{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykwn3Y4bLqmR"
   },
   "source": [
    "DATA COLLECTION ALGORITHMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qhMvNyXZGKpk"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Set the API endpoint URL\n",
    "url=\"https://jsonplaceholder.typicode.com/todos/1\"\n",
    "\n",
    "# Set any query parameters required by the API\n",
    "params={\"parameter1\":\"value1\",\"parameter2\":\"value2\"}\n",
    "\n",
    "# Send a GET request to the API endpoint with the query parameters\n",
    "response=requests.get(url,params=params)\n",
    "\n",
    "# Check if the response status code is 200 (OK)\n",
    "if response.status_code==200:\n",
    "    # Load the response JSON data as a dictionary\n",
    "    data=json.loads(response.text)\n",
    "\n",
    "    # Extract the data you need from the JSON dictionary\n",
    "    #chatbot_data = data[\"chatbot_data\"]\n",
    "\n",
    "    if \"chatbot_data\" in data:\n",
    "        chatbot_data=data[\"chatbot_data\"]\n",
    "    # handle the case when the key is not present\n",
    "    \n",
    "else:\n",
    "    # Handle the error response\n",
    "    print(\"Error:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HzzMLlpO7Pi9"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "# Define a function to perform NER on user input\n",
    "def perform_ner(input_text):\n",
    "    tokens = nltk.word_tokenize(input_text)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    entities = nltk.chunk.ne_chunk(tagged)\n",
    "    return entities\n",
    "\n",
    "# Example usage\n",
    "print(perform_ner(\"What is the sick leave policy?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQXqEeQ27nRE"
   },
   "outputs": [],
   "source": [
    "!pip install chatterbot\n",
    "try:\n",
    "  from chatterbot import ChatBot\n",
    "  from chatterbot.trainers import ChatterBotCorpusTrainer\n",
    "\n",
    "# Create a chatbot instance\n",
    "  chatbot = ChatBot('HR Chatbot')\n",
    "\n",
    "# Train the chatbot on HR-related data\n",
    "  trainer = ChatterBotCorpusTrainer(chatbot)\n",
    "  trainer.train(\"chatterbot.corpus.english.hr\")\n",
    "\n",
    "# Define a function to handle user input and generate a response\n",
    "  def get_response(input_text):\n",
    "    response = chatbot.get_response(input_text)\n",
    "    return str(response)\n",
    "\n",
    "  # Example usage\n",
    "  print(get_response(\"What is the sick leave policy?\"))\n",
    "except:\n",
    "  print(\"Error Has Occured! Try Again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VdtiAfzq8mSy",
    "outputId": "10129264-9af5-4017-e533-70a2751ab06c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required libraries and modules\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import os\n",
    "import psycopg2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#!pip install chatbot\n",
    "try:\n",
    "  from chatbot import Chatbot\n",
    "  # Define data sources\n",
    "  hr_policy_docs = \"path/to/hr_policy_docs\"\n",
    "  external_data = \"path/to/external_data\"\n",
    "\n",
    "  # Collect and preprocess data\n",
    "  def collect_data():\n",
    "      # Read HR policy documents\n",
    "      hr_docs = pd.read_csv(hr_policy_docs)\n",
    "      hr_docs = hr_docs.drop_duplicates()\n",
    "      # Read external data\n",
    "      ext_data = pd.read_csv(external_data)\n",
    "      # Combine HR and external data\n",
    "      combined_data = pd.concat([hr_docs, ext_data], ignore_index=True)\n",
    "      # Clean and preprocess data\n",
    "      combined_data['text'] = combined_data['text'].apply(lambda x: clean_data(x))\n",
    "      combined_data['text'] = combined_data['text'].apply(lambda x: preprocess_data(x))\n",
    "      return combined_data\n",
    "\n",
    "  # Clean data\n",
    "  def clean_data(text):\n",
    "      # Perform text cleaning operations to remove noise and irrelevant information from the text\n",
    "      return cleaned_text\n",
    "\n",
    "  # Preprocess data\n",
    "  def preprocess_data(text):\n",
    "      # Perform text preprocessing operations such as tokenization, stemming, and lemmatization\n",
    "      return preprocessed_text\n",
    "\n",
    "  # Label data\n",
    "  def label_data(data):\n",
    "      # Label the data with categories, tags, or keywords that correspond to the topics covered in the HR policy documents\n",
    "      return labeled_data\n",
    "\n",
    "  # Store data\n",
    "  def store_data(data):\n",
    "      # Store the cleaned and labeled data in a database or file system that can be easily accessed by the chatbot\n",
    "      return stored_data\n",
    "\n",
    "  # Update data\n",
    "  def update_data():\n",
    "      # Continuously update the data to ensure that the chatbot has access to the most up-to-date information\n",
    "      return updated_data\n",
    "\n",
    "  # Natural language processing\n",
    "  def nlp(text):\n",
    "      # Use NLP algorithms to process natural language queries from employees and extract relevant information from the knowledge base\n",
    "      return nlp_output\n",
    "\n",
    "  # Machine learning\n",
    "  def machine_learning(nlp_output):\n",
    "      # Develop machine learning algorithms that can learn from employee feedback and continuously improve the chat system's performance\n",
    "      return ml_output\n",
    "\n",
    "  # Chatbot development\n",
    "  def chatbot():\n",
    "      # Integrate the NLP and machine learning algorithms into a chatbot framework that can interact with employees in a conversational manner\n",
    "      return chatbot_output\n",
    "\n",
    "  # Feedback system\n",
    "  def feedback_system():\n",
    "      # Develop a feedback system that allows employees to rate the chatbot's responses and provide feedback on how to improve the system\n",
    "      return feedback_output\n",
    "\n",
    "  # Integration with backend\n",
    "  def backend_integration():\n",
    "      # Integrate the chatbot with the backend system, including the PostgreSQL database and file system, \n",
    "      #so that it can retrieve relevant information to answer employee queries\n",
    "      return backend_output\n",
    "\n",
    "  # Main function to run the entire algorithm\n",
    "  def main():\n",
    "      # Collect and preprocess data\n",
    "      data = collect_data()\n",
    "      \n",
    "      # Label data\n",
    "      labeled_data = label_data(data)\n",
    "      \n",
    "      # Store data\n",
    "      stored_data = store_data(labeled_data)\n",
    "      \n",
    "      # Continuously update data\n",
    "      while True:\n",
    "          updated_data = update_data()\n",
    "          \n",
    "          # Natural language processing\n",
    "          nlp_output = nlp(updated_data)\n",
    "          \n",
    "          # Machine learning\n",
    "          ml_output = machine_learning(nlp_output)\n",
    "          \n",
    "          # Chatbot development\n",
    "          chatbot_output = chatbot(ml_output)\n",
    "          \n",
    "          # Feedback system\n",
    "          feedback_output = feedback_system()\n",
    "\n",
    "except:\n",
    "  print(\"Error in importing chatbot package\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z66_Z7uoH502",
    "outputId": "e2fc2f6e-dbed-473d-ef28-add44601f256"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Define the API endpoint URL\n",
    "url = \"https://api.openweathermap.org/data/2.5/weather\"\n",
    "\n",
    "# Define the parameters for the API request\n",
    "params = {\n",
    "    \"q\": \"New York\",\n",
    "    \"appid\": \"your_api_key_here\",\n",
    "    \"units\": \"metric\"\n",
    "}\n",
    "\n",
    "# Send the API request\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "# Check if the request was successful (HTTP status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the response data as a JSON object\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the relevant data from the JSON object\n",
    "    temperature = data[\"main\"][\"temp\"]\n",
    "    humidity = data[\"main\"][\"humidity\"]\n",
    "    description = data[\"weather\"][0][\"description\"]\n",
    "\n",
    "    # Print the data\n",
    "    print(\"Temperature: {}Â°C\".format(temperature))\n",
    "    print(\"Humidity: {}%\".format(humidity))\n",
    "    print(\"Description: {}\".format(description))\n",
    "else:\n",
    "    # Handle the case where the API request was unsuccessful\n",
    "    print(\"API request failed with status code {}\".format(response.status_code))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvqUeE6GLiui"
   },
   "source": [
    "DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aQLu5oy3Lgsc",
    "outputId": "3dbc9341-c2aa-4d07-9af3-b60f18566965"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# define functions for text cleaning and preprocessing\n",
    "def clean_text(text):\n",
    "    text = text.lower() # convert to lowercase\n",
    "    text = re.sub(r'\\d+', '', text) # remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text) # remove extra whitespaces\n",
    "    return text.strip()\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "def preprocess(text):\n",
    "    text = clean_text(text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = lemmatize_tokens(tokens)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpxVBWWTL59c"
   },
   "source": [
    "NATURAL LANGUAGE PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7F9UegoxMf5e",
    "outputId": "810ac69b-b3b1-458c-a5ba-57ab4b5b5408"
   },
   "outputs": [],
   "source": [
    "#Step 1: Importing the necessary libraries\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "\n",
    "#Step 2: Reading the input data\n",
    "try:\n",
    "  f = open('chatbot.txt', 'r', errors = 'ignore')\n",
    "  raw = f.read()\n",
    "  raw = raw.lower()\n",
    "  nltk.download('punkt')\n",
    "  nltk.download('wordnet')\n",
    "  sent_tokens = nltk.sent_tokenize(raw)\n",
    "  word_tokens = nltk.word_tokenize(raw) \n",
    "except:\n",
    "  print(\"Error Occurred! Try Again\")\n",
    "\n",
    "#Step 3: Preprocessing the input data\n",
    "lemmer = WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "#Step 4: Defining the greeting function\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\", \"hey\",)\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"hello\", \"hey there\", \"hi there\", \"welcome\", \"hi, how can I help you?\"]\n",
    "def greeting(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)\n",
    "\n",
    "#Step 5: Generating a response\n",
    "try:\n",
    "  def response(user_response):\n",
    "    bot_response = ''\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx = vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf == 0):\n",
    "        bot_response = bot_response + \"I am sorry! I don't understand you\"\n",
    "        return bot_response\n",
    "    else:\n",
    "        bot_response = bot_response + sent_tokens[idx]\n",
    "        return bot_response\n",
    "except:\n",
    "  print(\"Error Occurred!Try Again\")\n",
    "\n",
    "#Step 6: Running the chatbot\n",
    "flag=True\n",
    "print(\"BOT: My name is Chatbot. I will answer your queries. If you want to exit, type Bye!\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response = user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"BOT: You are welcome..\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"BOT: \"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"BOT: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"BOT: Bye! take care..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gq5aSTRMMM_f"
   },
   "source": [
    "MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s3Gq-uA4NiIv",
    "outputId": "86aa43d3-be15-42f3-f406-38ceb0d0eb6e"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Download nltk packages\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Define a function to preprocess input text\n",
    "def preprocess(text):\n",
    "    # Tokenize text into words\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    # Remove punctuation and non-alphabetic characters\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # Lemmatize words\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    # Remove stopwords\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if not word in stopwords]\n",
    "    # Return preprocessed text as a string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Define some initial greetings and responses\n",
    "greetings = [\"hi\", \"hello\", \"hey\", \"howdy\", \"hola\"]\n",
    "responses = [\"hello\", \"hi there\", \"hi\", \"I'm glad you're talking to me!\"]\n",
    "\n",
    "# Load the chatbot's knowledge base\n",
    "try:\n",
    "  with open('knowledge.txt', 'r') as f:\n",
    "    knowledge = f.read()\n",
    "except:\n",
    "  print(\"Error Occurred! Try Again\")\n",
    "\n",
    "# Preprocess knowledge base\n",
    "try:\n",
    "  preprocessed_knowledge = preprocess(knowledge)\n",
    "except:\n",
    "  print(\"Error Occurred!Try Again\")\n",
    "\n",
    "# Create a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "# Generate document-term matrix\n",
    "try:\n",
    "  doc_term_matrix = vectorizer.fit_transform([preprocessed_knowledge])\n",
    "except:\n",
    "  print(\"Error Occurred!Try Again\")\n",
    "\n",
    "# Define a function to generate responses\n",
    "def generate_response(user_input):\n",
    "    # Preprocess user input\n",
    "    preprocessed_input = preprocess(user_input)\n",
    "    # Add user input to document-term matrix\n",
    "    doc_term_matrix_user = vectorizer.transform([preprocessed_input])\n",
    "    # Compute cosine similarity between user input and knowledge base\n",
    "    similarity = cosine_similarity(doc_term_matrix_user, doc_term_matrix)[0]\n",
    "    # Get index of most similar response\n",
    "    idx = np.argmax(similarity)\n",
    "    # Return most similar response\n",
    "    return knowledge.split('\\n')[idx]\n",
    "\n",
    "# Start the chatbot\n",
    "print(\"Hi, I'm an AI chatbot. What's on your mind?\")\n",
    "while True:\n",
    "    user_input = input(\"> \")\n",
    "    # Check for exit command\n",
    "    if user_input.lower() == 'bye':\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    # Check for greetings\n",
    "    if user_input.lower() in greetings:\n",
    "        print(random.choice(responses))\n",
    "    # Generate response\n",
    "    else:\n",
    "        print(generate_response(user_input))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mad-v-BMRIP"
   },
   "source": [
    "CHAT BOT DEVELOPMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nBZ-coBqN0B6",
    "outputId": "ce19e7b3-301e-4bd8-bc52-09091800b96e"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Define the possible responses for the chatbot\n",
    "greetings = [\"hello\", \"hi\", \"hey\", \"what's up\"]\n",
    "responses = {\n",
    "    \"hello\": \"Hello, how are you?\",\n",
    "    \"hi\": \"Hi there, how can I help you?\",\n",
    "    \"hey\": \"Hey, how's it going?\",\n",
    "    \"what's up\": \"Not much, what's up with you?\"\n",
    "}\n",
    "\n",
    "# Define a function to generate a response to a user input\n",
    "def get_response(user_input):\n",
    "    for word in user_input.split():\n",
    "        if word.lower() in greetings:\n",
    "            return random.choice(greetings)\n",
    "    if user_input.lower() in responses:\n",
    "        return responses[user_input.lower()]\n",
    "    else:\n",
    "        return \"I'm sorry, I don't understand what you're saying.\"\n",
    "\n",
    "# Define a loop to keep the chatbot running and responding to user input\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"quit\":\n",
    "        break\n",
    "    response = get_response(user_input)\n",
    "    print(\"Chatbot:\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OC_kgGv6MUmP"
   },
   "source": [
    "FEEDBACK SYSTEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GhiD2NUNODDp",
    "outputId": "caa45611-9c28-4c11-c005-4091863c0de4"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Define a list of possible responses to user feedback\n",
    "feedback_responses = [\n",
    "    \"Thank you for your feedback, we will take it into consideration.\",\n",
    "    \"We appreciate your input, it will help us improve.\",\n",
    "    \"Thanks for letting us know, we will use your feedback to make improvements.\",\n",
    "]\n",
    "\n",
    "# Define a function to handle user feedback\n",
    "def handle_feedback(feedback):\n",
    "    response = random.choice(feedback_responses)\n",
    "    print(response)\n",
    "\n",
    "# Example usage\n",
    "feedback = input(\"Please enter your feedback: \")\n",
    "handle_feedback(feedback)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q99PdIBZMWoh"
   },
   "source": [
    "INTEGRATION WITH BACKEND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WgTa8MJ5OTy_"
   },
   "outputs": [],
   "source": [
    "!pip install chatterbot\n",
    "!pip install ChatBot\n",
    "!pip install flask\n",
    "!pip install ChatterBotCorpusTrainer\n",
    "!pip install flask chatterbot\n",
    "from flask import Flask, render_template, request\n",
    "from chatterbot import ChatBot\n",
    "from chatterbot.trainers import ChatterBotCorpusTrainer\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Create a new chat bot\n",
    "bot = ChatBot('MyBot')\n",
    "\n",
    "# Create a new trainer for the chat bot\n",
    "trainer = ChatterBotCorpusTrainer(bot)\n",
    "\n",
    "# Train the chat bot with the English corpus\n",
    "trainer.train(\"chatterbot.corpus.english\")\n",
    "\n",
    "# Define the home page route\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return render_template(\"index.html\")\n",
    "\n",
    "# Define the chatbot API endpoint\n",
    "@app.route(\"/get\")\n",
    "def get_bot_response():\n",
    "    user_text = request.args.get('msg')\n",
    "    bot_response = str(bot.get_response(user_text))\n",
    "    return bot_response\n",
    "\n",
    "# Start the server\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yOBojxvdMbBX"
   },
   "source": [
    "TESTING AND DEPLOYEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WEukb7GAOxxn",
    "outputId": "9567f4dc-a321-4004-d49c-0155d1554ee6"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "\n",
    "# Define responses to greetings\n",
    "greeting_responses = [\"Hello!\", \"Hi there!\", \"Greetings!\", \"Good Evening!\"]\n",
    "\n",
    "# Define responses to user input\n",
    "user_input_responses = [\"I'm sorry, I didn't understand that.\", \"Could you please rephrase that?\", \"Can you be more specific?\"]\n",
    "\n",
    "# Define function to handle user input\n",
    "def respond_to_user_input(user_input):\n",
    "    # Check if user input contains a greeting\n",
    "    if \"hello\" in user_input.lower() or \"hi\" in user_input.lower():\n",
    "        return random.choice(greeting_responses)\n",
    "    # If user input doesn't contain a greeting, respond with a generic response\n",
    "    else:\n",
    "        return random.choice(user_input_responses)\n",
    "\n",
    "# Define main function\n",
    "def main():\n",
    "    print(\"Hello! I am an AI chatbot. How can I assist you today?\")\n",
    "    while True:\n",
    "        # Get user input\n",
    "        user_input = input(\"User: \")\n",
    "        # Exit loop if user input is \"bye\"\n",
    "        if user_input.lower() == \"bye\":\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "        # Respond to user input\n",
    "        chatbot_response = respond_to_user_input(user_input)\n",
    "        print(\"Chatbot: \" + chatbot_response)\n",
    "\n",
    "# Call main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsaWKhJ7QSvI"
   },
   "source": [
    "FULL AI CHAT BOT SOLUTION BY COMBINING ALL CODE WE HAVE : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OCrEcaDiQNCO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "82/82 [==============================] - 1s 2ms/step - loss: 3.6526 - accuracy: 0.0392\n",
      "Epoch 2/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 3.5481 - accuracy: 0.0907\n",
      "Epoch 3/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 3.3946 - accuracy: 0.1373\n",
      "Epoch 4/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 3.1617 - accuracy: 0.1961\n",
      "Epoch 5/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 2.9470 - accuracy: 0.2328\n",
      "Epoch 6/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 2.7459 - accuracy: 0.3088\n",
      "Epoch 7/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 2.5507 - accuracy: 0.3480\n",
      "Epoch 8/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 2.3436 - accuracy: 0.4093\n",
      "Epoch 9/200\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 2.2300 - accuracy: 0.4436\n",
      "Epoch 10/200\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 2.0084 - accuracy: 0.5000\n",
      "Epoch 11/200\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 1.9005 - accuracy: 0.4926\n",
      "Epoch 12/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 1.8187 - accuracy: 0.5147\n",
      "Epoch 13/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 1.7703 - accuracy: 0.5245\n",
      "Epoch 14/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 1.6237 - accuracy: 0.5809\n",
      "Epoch 15/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 1.5566 - accuracy: 0.5931\n",
      "Epoch 16/200\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 1.4122 - accuracy: 0.6348\n",
      "Epoch 17/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 1.3812 - accuracy: 0.6225\n",
      "Epoch 18/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 1.2729 - accuracy: 0.6716\n",
      "Epoch 19/200\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 1.1640 - accuracy: 0.7206\n",
      "Epoch 20/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 1.1383 - accuracy: 0.7034\n",
      "Epoch 21/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 1.0678 - accuracy: 0.7181\n",
      "Epoch 22/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 1.0568 - accuracy: 0.7083\n",
      "Epoch 23/200\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 1.0193 - accuracy: 0.7377\n",
      "Epoch 24/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.9834 - accuracy: 0.7328\n",
      "Epoch 25/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.9404 - accuracy: 0.7353\n",
      "Epoch 26/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.8988 - accuracy: 0.7402\n",
      "Epoch 27/200\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.9677 - accuracy: 0.7377\n",
      "Epoch 28/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.8515 - accuracy: 0.7794\n",
      "Epoch 29/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.8579 - accuracy: 0.7794\n",
      "Epoch 30/200\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.7648 - accuracy: 0.7794\n",
      "Epoch 31/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.8386 - accuracy: 0.7794\n",
      "Epoch 32/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.7505 - accuracy: 0.8064\n",
      "Epoch 33/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.7924 - accuracy: 0.7868\n",
      "Epoch 34/200\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.7815 - accuracy: 0.7672\n",
      "Epoch 35/200\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.6749 - accuracy: 0.8358\n",
      "Epoch 36/200\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.7152 - accuracy: 0.8113\n",
      "Epoch 37/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.6425 - accuracy: 0.8358\n",
      "Epoch 38/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.6298 - accuracy: 0.8407\n",
      "Epoch 39/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.6474 - accuracy: 0.8260\n",
      "Epoch 40/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.6335 - accuracy: 0.8186\n",
      "Epoch 41/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.6342 - accuracy: 0.8480\n",
      "Epoch 42/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.6709 - accuracy: 0.7990\n",
      "Epoch 43/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.6241 - accuracy: 0.8113\n",
      "Epoch 44/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.6141 - accuracy: 0.8260\n",
      "Epoch 45/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.6166 - accuracy: 0.8358\n",
      "Epoch 46/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.6574 - accuracy: 0.8309\n",
      "Epoch 47/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.5958 - accuracy: 0.8309\n",
      "Epoch 48/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.6083 - accuracy: 0.8382\n",
      "Epoch 49/200\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.5834 - accuracy: 0.8431\n",
      "Epoch 50/200\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.6213 - accuracy: 0.8358\n",
      "Epoch 51/200\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.5759 - accuracy: 0.8382\n",
      "Epoch 52/200\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.5996 - accuracy: 0.8407\n",
      "Epoch 53/200\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.5847 - accuracy: 0.8480\n",
      "Epoch 54/200\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.6016 - accuracy: 0.8309\n",
      "Epoch 55/200\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.5772 - accuracy: 0.8407\n",
      "Epoch 56/200\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.5171 - accuracy: 0.8456\n",
      "Epoch 57/200\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.5014 - accuracy: 0.8652\n",
      "Epoch 58/200\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.4997 - accuracy: 0.8652\n",
      "Epoch 59/200\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.5194 - accuracy: 0.8456\n",
      "Epoch 60/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.4907 - accuracy: 0.8652\n",
      "Epoch 61/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.4729 - accuracy: 0.8750\n",
      "Epoch 62/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.5411 - accuracy: 0.8529\n",
      "Epoch 63/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.5061 - accuracy: 0.8750\n",
      "Epoch 64/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.4956 - accuracy: 0.8652\n",
      "Epoch 65/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.4682 - accuracy: 0.8750\n",
      "Epoch 66/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.4735 - accuracy: 0.8652\n",
      "Epoch 67/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.4917 - accuracy: 0.8505\n",
      "Epoch 68/200\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.4886 - accuracy: 0.8627\n",
      "Epoch 69/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.5339 - accuracy: 0.8407\n",
      "Epoch 70/200\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.4734 - accuracy: 0.8652\n",
      "Epoch 71/200\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.4391 - accuracy: 0.8799\n",
      "Epoch 72/200\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.5095 - accuracy: 0.8505\n",
      "Epoch 73/200\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.4636 - accuracy: 0.8725\n",
      "Epoch 74/200\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.4752 - accuracy: 0.8505\n",
      "Epoch 75/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.4327 - accuracy: 0.8750\n",
      "Epoch 76/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.4617 - accuracy: 0.8701\n",
      "Epoch 77/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.4699 - accuracy: 0.8652\n",
      "Epoch 78/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.4570 - accuracy: 0.8922\n",
      "Epoch 79/200\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.4204 - accuracy: 0.8750\n",
      "Epoch 80/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.4535 - accuracy: 0.8701\n",
      "Epoch 81/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.4607 - accuracy: 0.8652\n",
      "Epoch 82/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.4426 - accuracy: 0.8750\n",
      "Epoch 83/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - 0s 2ms/step - loss: 0.4015 - accuracy: 0.8725\n",
      "Epoch 84/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3770 - accuracy: 0.8946\n",
      "Epoch 85/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.4058 - accuracy: 0.8946\n",
      "Epoch 86/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3813 - accuracy: 0.8897\n",
      "Epoch 87/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.4214 - accuracy: 0.8848\n",
      "Epoch 88/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.4214 - accuracy: 0.8652\n",
      "Epoch 89/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.4238 - accuracy: 0.8824\n",
      "Epoch 90/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.4273 - accuracy: 0.8824\n",
      "Epoch 91/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3911 - accuracy: 0.8848\n",
      "Epoch 92/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.4186 - accuracy: 0.8578\n",
      "Epoch 93/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3711 - accuracy: 0.8922\n",
      "Epoch 94/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3620 - accuracy: 0.9093\n",
      "Epoch 95/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3691 - accuracy: 0.8946\n",
      "Epoch 96/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.4193 - accuracy: 0.8995\n",
      "Epoch 97/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.4091 - accuracy: 0.8775\n",
      "Epoch 98/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.4179 - accuracy: 0.8799\n",
      "Epoch 99/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3619 - accuracy: 0.8946\n",
      "Epoch 100/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3846 - accuracy: 0.8799\n",
      "Epoch 101/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3939 - accuracy: 0.8725\n",
      "Epoch 102/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3819 - accuracy: 0.8775\n",
      "Epoch 103/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3754 - accuracy: 0.8824\n",
      "Epoch 104/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3486 - accuracy: 0.8873\n",
      "Epoch 105/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3396 - accuracy: 0.9044\n",
      "Epoch 106/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3701 - accuracy: 0.8971\n",
      "Epoch 107/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3850 - accuracy: 0.8701\n",
      "Epoch 108/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3501 - accuracy: 0.9020\n",
      "Epoch 109/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3719 - accuracy: 0.8995\n",
      "Epoch 110/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.4006 - accuracy: 0.8824\n",
      "Epoch 111/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3829 - accuracy: 0.8922\n",
      "Epoch 112/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3830 - accuracy: 0.8750\n",
      "Epoch 113/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3656 - accuracy: 0.8922\n",
      "Epoch 114/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3642 - accuracy: 0.8971\n",
      "Epoch 115/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3844 - accuracy: 0.8824\n",
      "Epoch 116/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3546 - accuracy: 0.8922\n",
      "Epoch 117/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3471 - accuracy: 0.9020\n",
      "Epoch 118/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3696 - accuracy: 0.8897\n",
      "Epoch 119/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3478 - accuracy: 0.8873\n",
      "Epoch 120/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3244 - accuracy: 0.9020\n",
      "Epoch 121/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.4091 - accuracy: 0.8775\n",
      "Epoch 122/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3276 - accuracy: 0.8971\n",
      "Epoch 123/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3406 - accuracy: 0.8922\n",
      "Epoch 124/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3712 - accuracy: 0.8897\n",
      "Epoch 125/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.4026 - accuracy: 0.8799\n",
      "Epoch 126/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3412 - accuracy: 0.8799\n",
      "Epoch 127/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3469 - accuracy: 0.8848\n",
      "Epoch 128/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3122 - accuracy: 0.9069\n",
      "Epoch 129/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3394 - accuracy: 0.8848\n",
      "Epoch 130/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3339 - accuracy: 0.8971\n",
      "Epoch 131/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3500 - accuracy: 0.8946\n",
      "Epoch 132/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3782 - accuracy: 0.8897\n",
      "Epoch 133/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3234 - accuracy: 0.8971\n",
      "Epoch 134/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3525 - accuracy: 0.8873\n",
      "Epoch 135/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3245 - accuracy: 0.9020\n",
      "Epoch 136/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3419 - accuracy: 0.8799\n",
      "Epoch 137/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3451 - accuracy: 0.8946\n",
      "Epoch 138/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3282 - accuracy: 0.9069\n",
      "Epoch 139/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3418 - accuracy: 0.9044\n",
      "Epoch 140/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3209 - accuracy: 0.9069\n",
      "Epoch 141/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3404 - accuracy: 0.8971\n",
      "Epoch 142/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3135 - accuracy: 0.9020\n",
      "Epoch 143/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3200 - accuracy: 0.9020\n",
      "Epoch 144/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2938 - accuracy: 0.9167\n",
      "Epoch 145/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3527 - accuracy: 0.8971\n",
      "Epoch 146/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3242 - accuracy: 0.8995\n",
      "Epoch 147/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3156 - accuracy: 0.9093\n",
      "Epoch 148/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3511 - accuracy: 0.9020\n",
      "Epoch 149/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3544 - accuracy: 0.8750\n",
      "Epoch 150/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3381 - accuracy: 0.8897\n",
      "Epoch 151/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2970 - accuracy: 0.9093\n",
      "Epoch 152/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3239 - accuracy: 0.9020\n",
      "Epoch 153/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3287 - accuracy: 0.8897\n",
      "Epoch 154/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3114 - accuracy: 0.9142\n",
      "Epoch 155/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3026 - accuracy: 0.9020\n",
      "Epoch 156/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3177 - accuracy: 0.8995\n",
      "Epoch 157/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3069 - accuracy: 0.8995\n",
      "Epoch 158/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2880 - accuracy: 0.9142\n",
      "Epoch 159/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3113 - accuracy: 0.8995\n",
      "Epoch 160/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3023 - accuracy: 0.8971\n",
      "Epoch 161/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3116 - accuracy: 0.9020\n",
      "Epoch 162/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3329 - accuracy: 0.8995\n",
      "Epoch 163/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3303 - accuracy: 0.9020\n",
      "Epoch 164/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3056 - accuracy: 0.9142\n",
      "Epoch 165/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2758 - accuracy: 0.9044\n",
      "Epoch 166/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3287 - accuracy: 0.8946\n",
      "Epoch 167/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3297 - accuracy: 0.8971\n",
      "Epoch 168/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3626 - accuracy: 0.8946\n",
      "Epoch 169/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3030 - accuracy: 0.9044\n",
      "Epoch 170/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3049 - accuracy: 0.9142\n",
      "Epoch 171/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3096 - accuracy: 0.9044\n",
      "Epoch 172/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3131 - accuracy: 0.9093\n",
      "Epoch 173/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3387 - accuracy: 0.9069\n",
      "Epoch 174/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3119 - accuracy: 0.9093\n",
      "Epoch 175/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2940 - accuracy: 0.9142\n",
      "Epoch 176/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3320 - accuracy: 0.8946\n",
      "Epoch 177/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3246 - accuracy: 0.8995\n",
      "Epoch 178/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3526 - accuracy: 0.8971\n",
      "Epoch 179/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3301 - accuracy: 0.9044\n",
      "Epoch 180/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.2973 - accuracy: 0.9118\n",
      "Epoch 181/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3182 - accuracy: 0.9069\n",
      "Epoch 182/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3056 - accuracy: 0.8946\n",
      "Epoch 183/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3319 - accuracy: 0.9020\n",
      "Epoch 184/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3268 - accuracy: 0.8995\n",
      "Epoch 185/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3043 - accuracy: 0.9069\n",
      "Epoch 186/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2905 - accuracy: 0.9044\n",
      "Epoch 187/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3511 - accuracy: 0.8946\n",
      "Epoch 188/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3046 - accuracy: 0.9069\n",
      "Epoch 189/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3207 - accuracy: 0.8971\n",
      "Epoch 190/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2777 - accuracy: 0.9167\n",
      "Epoch 191/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2988 - accuracy: 0.9044\n",
      "Epoch 192/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3191 - accuracy: 0.9093\n",
      "Epoch 193/200\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.2978 - accuracy: 0.9069\n",
      "Epoch 194/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3065 - accuracy: 0.8971\n",
      "Epoch 195/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3277 - accuracy: 0.8873\n",
      "Epoch 196/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3000 - accuracy: 0.9118\n",
      "Epoch 197/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3117 - accuracy: 0.9044\n",
      "Epoch 198/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3088 - accuracy: 0.9044\n",
      "Epoch 199/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2880 - accuracy: 0.9142\n",
      "Epoch 200/200\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2998 - accuracy: 0.8946\n",
      "Welcome to the chatbot! Start talking with the bot (type 'quit' to stop):\n",
      "You :quit\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "\n",
    "# !pip install numpy\n",
    "# !pip install tensorflow\n",
    "import os\n",
    "# print(os.getcwd())\n",
    "# print(os.listdir())\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import random\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Reading and preprocessing the data\n",
    "with open('intents.json') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = []\n",
    "labels = []\n",
    "docs_x = []\n",
    "docs_y = []\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        tokenized_words = nltk.word_tokenize(pattern)\n",
    "       # print(tokenized_words)\n",
    "        words.extend(tokenized_words)\n",
    "        docs_x.append(tokenized_words)\n",
    "        docs_y.append(intent['tag'])\n",
    "\n",
    "    if intent['tag'] not in labels:\n",
    "        labels.append(intent['tag'])\n",
    "\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w != '?']\n",
    "words = sorted(list(set(words)))\n",
    "labels = sorted(labels)\n",
    "\n",
    "# Step 2: Creating training and testing data\n",
    "training_data = []\n",
    "output_data = []\n",
    "out_empty = [0] * len(labels)\n",
    "\n",
    "for x, doc in enumerate(docs_x):\n",
    "    bag = []\n",
    "\n",
    "    for w in words:\n",
    "        if w in doc:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "\n",
    "    output_row = out_empty[:]\n",
    "    output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "    training_data.append(bag)\n",
    "    output_data.append(output_row)\n",
    "\n",
    "training_data = np.array(training_data)\n",
    "output_data = np.array(output_data)\n",
    "\n",
    "# Step 3: Creating the model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(training_data[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(output_data[0]), activation='softmax'))\n",
    "\n",
    "# Step 4: Compiling the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Step 5: Training the model\n",
    "model.fit(training_data, output_data, epochs=200, batch_size=5, verbose=1)\n",
    "\n",
    "# Step 6: Saving the model\n",
    "model.save('chatbot_model.h5')\n",
    "\n",
    "# Step 7: Loading the model\n",
    "model = tf.keras.models.load_model('chatbot_model.h5')\n",
    "\n",
    "# Step 8: Defining the chatbot function\n",
    "def chatbot():\n",
    "    print(\"Welcome to the chatbot! Start talking with the bot (type 'quit' to stop):\")\n",
    "    while True:\n",
    "        user_input  = input(\"You :\")\n",
    "        if user_input.lower() == 'quit':\n",
    "            break\n",
    "\n",
    "        tokenized_words = nltk.word_tokenize(user_input)\n",
    "        tokenized_words = [lemmatizer.lemmatize(w.lower()) for w in tokenized_words]\n",
    "\n",
    "        input_data = np.array([0] * len(words))\n",
    "        for w in tokenized_words:\n",
    "            if w in words:\n",
    "                input_data[words.index(w)] = 1\n",
    "\n",
    "        results = model.predict(np.array([input_data]))\n",
    "        results_index = np.argmax(results)\n",
    "        tag = labels[results_index]\n",
    "\n",
    "        for intent in data['intents']:\n",
    "            if intent['tag'] == tag:\n",
    "                responses = intent['responses']\n",
    "\n",
    "        print(random.choice(responses))\n",
    "\n",
    "# Running the chatbot function\n",
    "chatbot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
